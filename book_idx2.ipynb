{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, collections\n",
    "import numpy as np\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('samplefile.txt') as book:\n",
    "    read_book = book.read()\n",
    "words = nltk.word_tokenize(read_book)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'science', ',', 'also', 'known', 'data-driven', 'science', ',', 'interdisciplinary', 'field', 'scientific', 'methods', ',', 'processes', ',', 'systems', 'extract', 'knowledge', 'insights', 'data', 'various', 'forms', ',', 'either', 'structured', 'unstructured', ',', 'similar', 'data', 'mining', '.', 'Data', 'science', 'concept', 'unify', 'statistics', ',', 'data', 'analysis', 'related', 'methods', 'order', 'understand', 'analyze', 'actual', 'phenomena', 'data', '.', 'It', 'employs', 'techniques', 'theories', 'drawn', 'many', 'fields', 'within', 'broad', 'areas', 'mathematics', ',', 'statistics', ',', 'information', 'science', ',', 'computer', 'science', ',', 'particular', 'subdomains', 'machine', 'learning', ',', 'classification', ',', 'cluster', 'analysis', ',', 'data', 'mining', ',', 'databases', ',', 'visualization', '.', 'Turing', 'award', 'winner', 'Jim', 'Gray', 'imagined', 'data', 'science', 'fourth', 'paradigm', 'science', '(', 'empirical', ',', 'theoretical', ',', 'computational', 'data-driven', ')', 'asserted', '``', 'everything', 'science', 'changing', 'impact', 'information', 'technology', \"''\", 'data', 'deluge', '.', 'When', 'Harvard', 'Business', 'Review', 'called', 'The', 'Sexiest', 'Job', '21st', 'Century', 'term', 'became', 'buzzword', ',', 'often', 'applied', 'business', 'analytics', ',', 'even', 'arbitrary', 'use', 'data', ',', 'used', 'sexed-up', 'term', 'statistics', '.', 'While', 'many', 'university', 'programs', 'offer', 'data', 'science', 'degree', ',', 'exists', 'consensus', 'definition', 'curriculum', 'contents', '.', 'Because', 'current', 'popularity', 'term', ',', 'many', '``', 'advocacy', 'efforts', \"''\", 'surrounding', '.', 'Data', 'scientists', 'use', 'data', 'analytical', 'ability', 'find', 'interpret', 'rich', 'data', 'sources', ';', 'manage', 'large', 'amounts', 'data', 'despite', 'hardware', ',', 'software', ',', 'bandwidth', 'constraints', ';', 'merge', 'data', 'sources', ';', 'ensure', 'consistency', 'datasets', ';', 'create', 'visualizations', 'aid', 'understanding', 'data', ';', 'build', 'mathematical', 'models', 'using', 'data', ';', 'present', 'communicate', 'data', 'insights/findings', '.', 'They', 'often', 'expected', 'produce', 'answers', 'days', 'rather', 'months', ',', 'work', 'exploratory', 'analysis', 'rapid', 'iteration', ',', 'produce', 'present', 'results', 'dashboards', '(', 'displays', 'current', 'values', ')', 'rather', 'papers/reports', ',', 'statisticians', 'normally', '.', 'Data', 'science', 'technology', 'mathematic', 'effective', 'data', 'scientists', 'require', 'combination', 'technical', 'skills', 'soft', 'skills', 'turn', 'data', 'actionable', 'insight', '.', 'Data', 'scientist', 'become', 'popular', 'occupation', 'Harvard', 'Business', 'Review', 'dubbing', 'The', 'Sexiest', 'Job', '21st', 'Century', 'McKinsey', '&', 'Company', 'projecting', 'global', 'excess', 'demand', '1.5', 'million', 'new', 'data', 'scientists', '.', 'Universities', 'offering', 'masters', 'courses', 'data', 'science', '.', 'Shorter', 'private', 'bootcamps', 'also', 'offering', 'data', 'science', 'certificates', 'including', 'student-paid', 'programs', 'like', 'General', 'Assembly', 'employer-paid', 'programs', 'like', 'The', 'Data', 'Incubator']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = []\n",
    "\n",
    "for w in words:\n",
    "    #w.lower()\n",
    "    if w not in stop_words:\n",
    "        filtered_words.append(w)\n",
    "print filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data', 'science', 'also', 'known', 'datadriven', 'science', 'interdisciplinary', 'field', 'scientific', 'methods', 'processes', 'systems', 'extract', 'knowledge', 'insights', 'data', 'various', 'forms', 'either', 'structured', 'unstructured', 'similar', 'data', 'mining', 'Data', 'science', 'concept', 'unify', 'statistics', 'data', 'analysis', 'related', 'methods', 'order', 'understand', 'analyze', 'actual', 'phenomena', 'data', 'It', 'employs', 'techniques', 'theories', 'drawn', 'many', 'fields', 'within', 'broad', 'areas', 'mathematics', 'statistics', 'information', 'science', 'computer', 'science', 'particular', 'subdomains', 'machine', 'learning', 'classification', 'cluster', 'analysis', 'data', 'mining', 'databases', 'visualization', 'Turing', 'award', 'winner', 'Jim', 'Gray', 'imagined', 'data', 'science', 'fourth', 'paradigm', 'science', 'empirical', 'theoretical', 'computational', 'datadriven', 'asserted', 'everything', 'science', 'changing', 'impact', 'information', 'technology', 'data', 'deluge', 'When', 'Harvard', 'Business', 'Review', 'called', 'The', 'Sexiest', 'Job', '21st', 'Century', 'term', 'became', 'buzzword', 'often', 'applied', 'business', 'analytics', 'even', 'arbitrary', 'use', 'data', 'used', 'sexedup', 'term', 'statistics', 'While', 'many', 'university', 'programs', 'offer', 'data', 'science', 'degree', 'exists', 'consensus', 'definition', 'curriculum', 'contents', 'Because', 'current', 'popularity', 'term', 'many', 'advocacy', 'efforts', 'surrounding', 'Data', 'scientists', 'use', 'data', 'analytical', 'ability', 'find', 'interpret', 'rich', 'data', 'sources', 'manage', 'large', 'amounts', 'data', 'despite', 'hardware', 'software', 'bandwidth', 'constraints', 'merge', 'data', 'sources', 'ensure', 'consistency', 'datasets', 'create', 'visualizations', 'aid', 'understanding', 'data', 'build', 'mathematical', 'models', 'using', 'data', 'present', 'communicate', 'data', 'insightsfindings', 'They', 'often', 'expected', 'produce', 'answers', 'days', 'rather', 'months', 'work', 'exploratory', 'analysis', 'rapid', 'iteration', 'produce', 'present', 'results', 'dashboards', 'displays', 'current', 'values', 'rather', 'papersreports', 'statisticians', 'normally', 'Data', 'science', 'technology', 'mathematic', 'effective', 'data', 'scientists', 'require', 'combination', 'technical', 'skills', 'soft', 'skills', 'turn', 'data', 'actionable', 'insight', 'Data', 'scientist', 'become', 'popular', 'occupation', 'Harvard', 'Business', 'Review', 'dubbing', 'The', 'Sexiest', 'Job', '21st', 'Century', 'McKinsey', 'Company', 'projecting', 'global', 'excess', 'demand', '15', 'million', 'new', 'data', 'scientists', 'Universities', 'offering', 'masters', 'courses', 'data', 'science', 'Shorter', 'private', 'bootcamps', 'also', 'offering', 'data', 'science', 'certificates', 'including', 'studentpaid', 'programs', 'like', 'General', 'Assembly', 'employerpaid', 'programs', 'like', 'The', 'Data', 'Incubator']\n"
     ]
    }
   ],
   "source": [
    "#Removal of space\n",
    "for w in filtered_words:\n",
    "    w.replace(\" \", \"\")\n",
    "\n",
    "#Removal of puncutation marks\n",
    "import string\n",
    "filtered_words = [''.join(c for c in s if c not in string.punctuation) for s in filtered_words]\n",
    "\n",
    "import string\n",
    "processed_word = []\n",
    "for each in filtered_words:\n",
    "    if not each:\n",
    "        continue\n",
    "    else:\n",
    "        processed_word.append(each) \n",
    "print(processed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['21st Century', 'Business Review', 'Harvard Business', 'Job 21st', 'Sexiest Job', 'The Sexiest', 'programs like', 'data mining', 'data sources', 'use data', 'Data science', 'data scientists', 'data science']\n"
     ]
    }
   ],
   "source": [
    "bigram = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(processed_word)\n",
    "#To find frequently together occuring 2 words\n",
    "finder.apply_freq_filter(2)\n",
    "\n",
    "bigram_word = []\n",
    "bigram_words = finder.nbest(bigram.pmi, 20)\n",
    "for each in bigram_words:\n",
    "    bigram_word.append(each[0]+\" \"+each[1])\n",
    "print bigram_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "word_list.extend(processed_word)\n",
    "word_list.extend(bigram_word)\n",
    "\n",
    "\n",
    "frequency = collections.Counter()\n",
    "for w in word_list:\n",
    "    frequency[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word  pos  wordcount\n",
      "0         concept   NN          1\n",
      "1     surrounding  VBG          1\n",
      "2             Jim  NNP          1\n",
      "3         results  NNS          1\n",
      "4         sources  NNS          2\n",
      "5       bandwidth   JJ          1\n",
      "6       scientist   NN          1\n",
      "7       including  VBG          1\n",
      "8              15   CD          1\n",
      "9        imagined   JJ          1\n",
      "10     occupation   JJ          1\n",
      "11         impact   NN          1\n",
      "12  visualization   NN          1\n",
      "13           21st   CD          2\n",
      "14          rapid   JJ          1\n",
      "15      knowledge   NN          1\n",
      "16       Business  NNP          2\n",
      "17          field   NN          1\n",
      "18    studentpaid  VBD          1\n",
      "19        Company  NNP          1\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(frequency.keys())\n",
    "\n",
    "notpos = ['VB','VBP','PRP','IN','RB','DT','WDT','WP','WRB','UH','TO','RBR','RBS','POS','MD','EX']\n",
    "w1 = list(filter(lambda word_tag: word_tag[1]  not in notpos, tagged))\n",
    "\n",
    "word = []\n",
    "pos = []\n",
    "count = []\n",
    "df =pd.DataFrame()\n",
    "\n",
    "dict ={}\n",
    "for each in w1:\n",
    "    #dict[each[0]]=each[1]\n",
    "    word.append(each[0])\n",
    "    pos.append(each[1])\n",
    "df['word']=word\n",
    "df['pos']=pos\n",
    "for each in word:\n",
    "    count.append(frequency[each])\n",
    "df['wordcount'] = count\n",
    "\n",
    "print df.head(20)\n",
    "print len(df['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['NN','NNP','NNS','VBG','VBD','VBN','VBZ','VBP','VB','CD','CC','LS','JJ','JJS','JJR','PDT','PRP','RP']\n",
    "df1 = pd.DataFrame(0, index=np.arange(len(df['word'])), columns=cols)\n",
    "\n",
    "df2 = pd.concat([df, df1], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, row in df2.iterrows():\n",
    "    pos = row['pos']\n",
    "    df2.set_value(idx, pos, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word  pos  wordcount  NN  NNP  NNS  VBG  VBD  VBN  VBZ ...  VB  CD  \\\n",
      "0      concept   NN          1   1    0    0    0    0    0    0 ...   0   0   \n",
      "1  surrounding  VBG          1   0    0    0    1    0    0    0 ...   0   0   \n",
      "2          Jim  NNP          1   0    1    0    0    0    0    0 ...   0   0   \n",
      "3      results  NNS          1   0    0    1    0    0    0    0 ...   0   0   \n",
      "4      sources  NNS          2   0    0    1    0    0    0    0 ...   0   0   \n",
      "5    bandwidth   JJ          1   0    0    0    0    0    0    0 ...   0   0   \n",
      "6    scientist   NN          1   1    0    0    0    0    0    0 ...   0   0   \n",
      "7    including  VBG          1   0    0    0    1    0    0    0 ...   0   0   \n",
      "8           15   CD          1   0    0    0    0    0    0    0 ...   0   1   \n",
      "9     imagined   JJ          1   0    0    0    0    0    0    0 ...   0   0   \n",
      "\n",
      "   CC  LS  JJ  JJS  JJR  PDT  PRP  RP  \n",
      "0   0   0   0    0    0    0    0   0  \n",
      "1   0   0   0    0    0    0    0   0  \n",
      "2   0   0   0    0    0    0    0   0  \n",
      "3   0   0   0    0    0    0    0   0  \n",
      "4   0   0   0    0    0    0    0   0  \n",
      "5   0   0   1    0    0    0    0   0  \n",
      "6   0   0   0    0    0    0    0   0  \n",
      "7   0   0   0    0    0    0    0   0  \n",
      "8   0   0   0    0    0    0    0   0  \n",
      "9   0   0   1    0    0    0    0   0  \n",
      "\n",
      "[10 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### done so far. kept the Rake eg below untouched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rake Example:\n",
    "Below is a sample code that gets ranked phrases from rake. This is mainly used to generate keyphrases. We can fine tune this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11 ][ 12 ][ 13 ][ 14 ][ 15', 'army air corps used 13 49th bomb squadron b', '10b supply flights followed earlier supply flights', 'distance bomber design competition 1934 – 1935', 'two chinese nationalist air force b', 'united states entered world war ii', '14th international volunteer squadron prepared', 'additional 103 aircraft designated b', 'aircraft dropped propaganda leaflets', 'much bigger boeing b', 'south east asia suffered', 'martin company fully expected', 'existing bombers completely obsolete', 'early war medium bombers', '7 ][ 8', 'several air forces', '9 ][ 10', 'two french pilots', '9th bomb group', '2d bomb group', '19th bomb group', 'netherlands east indies', 'martin received permission', 'martin model 146', 'model 139 design', 'severe winter storm', 'panama canal zone', 'late 1930s meant', 'large twin floats', 'islanders faced starvation', 'goodyear blimp enterprise', 'general henry h', 'dutch martins fought', '6th composite group', '4th composite group', 'six model 139ws', '20 model 139ws', 'g3 cyclone engines', '19 may 1938', '10 would flood', 'dropping incendiary bombs', '10s successfully flew', 'export model 139s', 'squadron ′', '10 february 1936', '2 february 1936', 'bomber design', 'medium bombers', 'japanese war', 'latest b', 'keystone b', 'douglas b', 'army owned', 'army ordered', 'july 1935', 'bomber role', 'biplane bomber', 'february 1938', 'early 1942', 'dropping bombs', '6a bombers', 'february 1936', 'g2 engines', 'model 166', '10 version', '10 began', 'virginia ′', 'usaac long', 'tangier island', 'smith island', 'ships unable', 'shipments began', 'september 1937', 'rapid advances', 'project submitted', 'mitchel field', 'minor changes', 'maryland ′', 'march field', 'latest fighters', 'langley field', 'highest performance', 'heavy ice', 'hardly superior', 'followed', 'export orders', 'enough armour', 'drop supplies', 'delivered versions', 'conventional duties', 'contemporary fighter', 'coastal patrol', 'citation needed', 'chesapeake bay', 'arnold described', 'april 1937', 'airpower wonder', 'actually inferior', 'abortive effort', '10b', 'b', 'wright r', 'modified yb', 'islands due', 'entered', 'army', 'advanced performance', '10bs served', '1935', 'design', '10', '1936', '9', '2', '139s', '10s', 'yb', 'r', 'orders', 'islands', 'advanced', '166', '10bs', 'u', 'turkey', 'time', 'theatre', 'sold', 'sino', 'singapore', 'siam', 'rights', 'revolution', 'residents', 'refused', 'reach', 'rather', 'raid', 'powered', 'philippines', 'outrun', 'operated', 'nevertheless', 'modernize', 'made', 'lost', 'legacy', 'kagoshima', 'japan', 'however', 'half', 'guns', 'filled', 'faster', 'fast', 'example', 'eclipsed', 'e', 'disadvantages', 'defense', 'day', 'creation', 'combat', 'china', 'addition', '1820', '18', '17', '16', '12as']\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text(read_book)\n",
    "print(r.get_ranked_phrases())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
