%%chapter%% 04
<%
  require "./scripts/eruby_util.rb"
%>

<%
  chapter(
    '04',
    %q{More about limits; curve sketching},
    'ch:more-limits'
  )
%>

<% begin_sec("Properties of the limit",nil,'properties-of-the-limit') %>\index{limit!properties}
In ch.~\ref{ch:limits} we did very few direct computations of limits using the epsilon-delta definition.
Epsilon-delta proofs are hard work, and by building up a more sophisticated
set of tools we can usually avoid having to apply the epsilon-delta definition directly.

<% begin_sec("Limits of constants and of $x$",nil,'limits-of-constants-and-x') %>
If $a$ and $c$ are constants,
then
\begin{equation}
  \lim_{x\to a}c=c \tag{$P_1$}
\end{equation}
and
\begin{equation}
  \lim_{x\to a} x= a.\tag{$P_2$}
\end{equation}
<% end_sec('limits-of-constants-and-x') %>

<% begin_sec("Limits of sums, products and quotients",nil,'limits-of-sum-prod-quot') %>
Let $F_1$ and $F_2$ be
two given functions whose limits for $x\to a$ we know,
\[
\lim_{x\to a}F_1(x)=L_1, \qquad \lim_{x\to a}F_2(x)=L_2.
\]
Then
\begin{align}
  \lim_{x\to a}\bigl(F_1(x)+F_2(x)\bigr)=L_1+L_2, \tag{$P_3$} \\
  \lim_{x\to a}\bigl(F_1(x)-F_2(x)\bigr)= L_1 - L_2, \tag{$P_4$} \\
  \lim_{x\to a}\bigl(F_1(x)\cdot F_2(x)\bigr)= L_1\cdot L_2 \tag{$P_5$}
\end{align}
Finally, if $ \lim_{x\to a}F_2(x)\ne0$,
\begin{equation}
  \lim_{x\to a}\frac{F_1(x)}{F_2(x)}= \frac{L_1}{L_2}.\tag{$P_6$}
\end{equation}
   
In other words the limit of the sum is the sum of the limits, etc.  One can
prove these laws using the definition of the limit, but we will not do this here.  However, I hope
these laws seem like common sense: if, for $x$ close to $a$, the quantity
$F_1(x)$ is close to $L_1$ and $F_2(x)$ is close to $L_2$, then certainly
$F_1(x)+F_2(x)$ should be close to $L_1+L_2$.

\begin{eg}{}
In this example we compute several limits, building up from simple examples to
more complicated ones.

First let's evaluate  $\lim_{x\to2}x^2$.
We have
\begin{align*}
  \lim_{x\to2} x^2 &= \lim_{x\to2} x\cdot x \\
  &= \bigl( \lim_{x\to2} x\bigr)\cdot \bigl( \lim_{x\to2} x\bigr)
  &\text{by $(P_5)$}\\
  &= 2\cdot 2 = 4.
\end{align*}
Similarly,
\begin{align*}
  \lim_{x\to2} x^3 &= \lim_{x\to2} x\cdot x^2 \\
  &= \bigl( \lim_{x\to2} x\bigr)\cdot \bigl( \lim_{x\to2} x^2\bigr)
  &\text{$(P_5)$ again}\\
  &= 2\cdot 4 = 8,
\end{align*}
and, by $(P_4)$
\[
\lim_{x\to2} x^2-1 = \lim_{x\to2} x^2 - \lim_{x\to2} 1 = 4-1 = 3,
\]
and, by $(P_4)$ again,
\[
\lim_{x\to2} x^3-1 = \lim_{x\to2} x^3 - \lim_{x\to2} 1 = 8-1 = 7,
\]
Putting all this together, we get
\[
\lim_{x\to 2}\frac{x^3-1}{x^2-1} = \frac{2^3-1}{2^2-1} = \frac{8-1}{4-1}=
\frac{7}{3}
\]
because of $(P_6)$.  To apply $(P_6)$ we must check that the denominator
(``$L_2$'') is not zero.  Since the denominator is 3, this was all right.
\end{eg}

\begin{eg}{The limit of a square root}\label{eg:limit-of-sqrt}
\egquestion Find $\lim_{x\to 2}\sqrt x$.

\eganswer Of course, you would think that $\lim_{x\to 2}\sqrt x = \sqrt 2$ and you
can indeed prove this using $\delta$ \& $\varepsilon$. But is there an easier way?  There is
nothing in the limit properties which tells us how to deal with a square
root, and using them we can't even prove that there is a limit.  However,
if you \emph{assume} that the limit exists then the limit properties allow
us to find this limit.

The argument goes like this: suppose that there is a number $L$ with
\[
\lim_{x\to 2} \sqrt x = L.
\]
Then property $(P_5)$ implies that
\[
L^2 = \bigl(\lim_{x\to2}\sqrt x\bigr)\cdot\bigl(\lim_{x\to2}\sqrt x\bigr) =
\lim_{x\to2} \sqrt x\cdot \sqrt x =\lim_{x\to2}x =2.
\]
In other words, $L^2=2$, and hence $L$ must be either $\sqrt 2$ or $-\sqrt
2$.  We can reject the latter because whatever $x$ does, its square root is
always a positive number, and hence it can never ``get close to'' a
negative number like $-\sqrt 2$.


Our conclusion: if the limit exists, then
\[
\lim_{x\to2} \sqrt x = \sqrt 2.
\]
The result is not surprising: if $x$ gets close to 2 then $\sqrt x$ gets
close to $\sqrt 2$.
\end{eg}
<% end_sec('limits-of-sum-prod-quot') %>

<% end_sec('properties-of-the-limit') %>
<% begin_sec("When limits fail to exist",nil,'when-limits-fail-to-exist') %>\index{limit!failure to exist}
In example \ref{eg:limit-of-sqrt} we worried about the possibility that a
limit $\lim_{x\to a}g(x)$ actually might not exist.  This can actually
happen, and in this section we'll see a few examples of what failed limits
look like.  First let's agree on what we will call a ``failed limit.''

\begin{lessimportant}
If there is no number $L$ such that $\lim_{x\to a}f(x) = L$, then we say
that the limit $\lim_{x\to a}f(x)$ does not exist. 
\end{lessimportant}

<% marg(0) %>
<%
  fig(
    'sign-function',
    %q{The sign function.}
  )  
%>
<% end_marg %>

\begin{eg}{The sign function near $x=0$}\label{eg:limit-of-sign-function}
The ``sign function'' is defined by
\[
\operatorname{sign} (x) =
\begin{cases}
  -1 & \text{for $x<0$}\\ 0 & \text{for $x=0$}\\ 1 & \text{for $x>0$}
\end{cases}
\]
Note that ``the sign of zero'' is defined to be zero.  But does the sign
function have a limit at $x=0$, i.e.\ does $ \lim_{x\to0} \operatorname{sign}(x) $ exist?
And is it also zero? The answers are \emph{no} and \emph{no}, and here is
why: suppose that for some number $L$ one had
\[
  \lim_{x\to 0}\operatorname{sign}(x) = L,
\]
then since for arbitrary small positive values of $x$ one has $\operatorname{sign}(x) =
+1$ one would think that $L=+1$.  But for arbitrarily small negative values
of $x$ one has $\operatorname{sign}(x)=-1$, so one would conclude that $L=-1$.  But one
number $L$ can't be both $+1$ and $-1$ at the same time, so there is no
such $L$, i.e.\ there is no limit.
\[
\lim_{x\to 0} \operatorname{sign} (x) \text{ does not exist.}
\]

In examples like this one, it is possible to define a one-sided limit; see
section \ref{subsec:one-sided-limits}.
\end{eg}

%%graph%% backward-sine-raw func=sin(3.1416/x) format=svg xlo=0.02 xhi=3 ylo=-1 yhi=1 with=lines samples=1000

<%
  fig(
    'backward-sine',
    %q{Example }+ref_workaround('eg:backward-sine')+".",
    #%q{Example \ref{eg:backward-sine}.},
    {
      'width'=>'fullpage'
    }
  )
%>

\begin{eg}{The ``backward sine''}\label{eg:backward-sine}
Figure \figref{backward-sine} shows the  ``backward sine'' function
$f(x)=\sin(\pi/x)$.
Contemplate its limit as $x\to0$:
\[
\lim_{x\to 0}\sin \bigl(\frac\pi x\bigr)\eqquad.
\]

When $x=0$ the function $f(x)$ is not defined, because its
definition involves division by $x$.  What happens to $f(x)$ as $x\to0$?
First, $\pi /x$ becomes larger and larger (``goes to infinity'') as
$x\to0$.  Then, taking the sine, we see that $ \sin(\pi /x)$ oscillates
between $+1$ and $-1$ infinitely often as $x\to0$.  This means that $f(x)$
gets close to any number between $-1$ and $+1$ as $x\to0$, but that the
function $f(x)$ \emph{never stays close} to any particular value because it
keeps oscillating up and down. The limit fails to exist, but for a different
reason than in example \ref{eg:limit-of-sign-function}.
\end{eg}

\begin{eg}{Trying to divide by zero using a limit}\label{eg:limit-to-divide-by-zero}
The expression $1/0$
is not defined, but what about
\[
\lim_{x\to0}\frac1x?
\]
This limit also does not exist.  Here are two reasons:

It is common wisdom that if you divide by a small number you get a large
number, so as $x\searrow 0$ the quotient $1/x$ will not be able to stay
close to any particular finite number, and the limit can't exist.

``Common wisdom'' is not always a reliable tool in mathematical proofs, so
here is a better argument.  The limit can't exist, because that would
contradict the limit properties $(P_1)\cdots(P_6)$.  Namely, suppose that
there were an number $L$ such that
\[
\lim_{x\to0} \frac 1 x = L.
\]
Then the limit property $(P_5)$ would imply that
\[
\lim_{x\to0}\bigl(\frac 1x\cdot x \bigr) = \bigl(\lim_{x\to0}\frac
1x\bigr)\cdot \bigl(\lim_{x\to0} x\bigr) = L\cdot 0 =0.
\]
On the other hand $\frac 1x \cdot x =1$ so the above limit should be 1!  A                                       
number can't be both 0 and 1 at the same time, so we have a
contradiction. The assumption that $\lim_{x\to0}1/x$ exists is to blame, so
it must go.
\end{eg}

<% begin_sec("Using limit properties to show a limit does \\emph{not} exist",nil,'props-to-show-no-limit') %>
The limit properties tell us how to prove that certain limits exist (and
how to compute them).  Although it is perhaps not so obvious at first
sight, they also allow you to prove that certain limits do not exist. 
Example \ref{eg:limit-to-divide-by-zero} shows one instance of such use.  Here is another.

Property $(P_3)$ says that if both $\lim_{x\to a}g(x)$ and $\lim_{x\to a}
h(x) $ exist then $\lim_{x\to a}g(x)+h(x)$ also must exist.  You can turn
this around and say that if $\lim_{x\to a}g(x)+h(x)$ does not exist then
either $\lim_{x\to a}g(x)$ or $\lim_{x\to a }h(x)$ does not exist (or both
limits fail to exist).

For instance, the limit
\[
\lim_{x\to 0} \frac 1x-x
\]
can't exist, for if it did, then the limit
\[
\lim_{x\to0} \frac 1x =\lim_{x\to 0} \bigl(\frac1x-x +x\bigr) =\lim_{x\to
0} \bigl(\frac1x-x\bigr) + \lim_{x\to 0} x
\]
would also have to exist, and we know $\lim_{x\to 0}\frac1x$ doesn't exist.

<% end_sec('props-to-show-no-limit') %>
<% end_sec('when-limits-fail-to-exist') %>

<% begin_sec("Variations on the theme of the limit",nil,'limit-variations') %>

Not all limits are ``for $x\to a$''.  Here we describe some variations on the
concept of limit.

<% begin_sec("Left and right limits",nil,'one-sided-limits') %>\index{limit!left and right}
When we let ``$x$ approach $a$'' we allow $x$ to be larger or smaller 
than $a$, as long as $x$ ``gets close to $a$''.  If we explicitly want to study
the behavior of $f(x)$ as $x$ approaches $a$ through values larger than
$a$, then we write
\[
\lim_{x\searrow a} f(x)\text{ or } \lim_{x\to a+} f(x) \text{ or }
\lim_{x\to a+0} f(x) \text{ or } \lim_{x\to a, x>a} f(x).
\]
All four notations are commonly used.  Similarly, to designate the value which  
$f(x)$ approaches as $x$ approaches $a$ through values below $a$ one writes
\[
\lim_{x\nearrow a} f(x)\text{ or } \lim_{x\to a-} f(x) \text{ or }
\lim_{x\to a-0} f(x) \text{ or } \lim_{x\to a, x<a} f(x).
\]
The precise definition of these ``one-sided'' limits goes like this:

\begin{important}[Definition of right- and left-limits]
Let $f$ be a function.  Then the right-limit notation
\begin{equation}\label{eqn:one-sided-lim-formulation}
  \lim_{x\searrow a} f(x) = L\eqquad.
\end{equation}
means that for every $\varepsilon>0$ one can find a $\delta>0$ such that
\[
a<x<a+\delta \implies |f(x)-L|<\varepsilon
\]
holds for all $x$ in the domain of $f$.

The definition of a left-limit is exactly analogous.
When we say
\begin{equation}\label{eqn:left-sided-lim-formulation}
  \lim_{x\nearrow a} f(x) = L\eqquad,
\end{equation}
we mean that for every $\varepsilon>0$ one can find a $\delta>0$ such that
\[
a-\delta<x<a \implies |f(x)-L|<\varepsilon
\]
holds for all $x$ in the domain of $f$.
\end{important}

The following
theorem tells you how to use one-sided limits to decide if a function
$f(x)$ has a limit at $x=a$.

\begin{theorem}
The two-sided limit $\displaystyle \lim_{x\to a} f(x)$
exists if and only if the two one-sided limits
\[
  \lim_{x\searrow a} f(x), \quad\text{and}\quad \lim_{x\nearrow a} f(x)
\]
exist and have the same value.
\end{theorem}

<% end_sec('one-sided-limits') %>

<% begin_sec("Limits at infinity",nil,'limits-at-infinity') %>\index{limit!at infinity}
So far we have defined the limit of a function $f(x)$ as $x$ gets closer and closer to some
finite value. It can also be of interest to let $x$ become
``larger and larger'' and ask what happens to $f(x)$.  If there is a number
$L$ such that $f(x)$ gets arbitrarily close to $L$ if one chooses $x$
sufficiently large, then we write   
\[
\lim_{x\to \infty} f(x) = L
\]
(``The limit for $x$ going to infinity is $L$.'')
We have an analogous definition for what happens to $f(x)$ as $x$ becomes very
large and negative: we write
\[
\lim_{x\to -\infty} f(x) = L  
\]
(``The limit for $x$ going to negative infinity is $L$.'')

Here are the precise definitions:

\begin{important}[Definitions of limits at infinity]\index{infinite and infinitesimal quantities!limits at infinity}
Let $f(x)$ be a function which is defined on an interval $x_0<x<\infty$.
If there is a number $L$ such that for every $\varepsilon>0$ we can find
an $A$ such that
\[
x>A \implies |f(x) - L| <\varepsilon
\]
for all $x$, then we say that the limit of $f(x)$ for $x\to\infty$ is $L$.

Similarly, let $f(x)$ be a function which is defined on an interval $-\infty < x < x_0$.
If there is a number $L$ such that for every $\varepsilon>0$ we can find
an $A$ such that
\[
x<-A \implies |f(x) - L| <\varepsilon
\]
for all $x$, then we say that the limit of $f(x)$ for $x\to-\infty$ is $L$.
\end{important}

These definitions are very similar to the original definition of the limit
in section \ref{sec:limit} on p.~\pageref{sec:limit}.
Instead of $\delta$ which specifies how close $x$ should be to $a$, we now
have a number $A$ that says how large $x$ should be, which is a way of
saying ``how close $x$ should be to infinity'' (or to negative infinity).

But although these definitions are similar to the original one, they are not
quite the same.
Note that there is no real number called $\infty$, and therefore we can't just
take the definition of $\lim_{x\rightarrow a}$ and substitute
$\infty$ for $a$. (Cf.~rule 2 on p.~\pageref{infinity-is-not-a-hyperreal}.)

%%graph%% dying-vibration-raw func=cos(3.14*x)*(0.7)**(x/6) format=svg xlo=0.0 xhi=18 ylo=-1 yhi=1 with=lines samples=1000

<%
  fig(
    'dying-vibration',
    %q{The value of $A$ is large enough for the given $\varepsilon$. The graph could represent the
       dying vibration of a gong as a function of time. Because we can find such an $A$ for every $\varepsilon$, the
       vibration dies out to zero as time approaches infinity.},
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

\begin{eg}{The limit of $1/x$}
The larger $x$ is, the smaller its reciprocal, so it seems natural that $1/x
\to 0$ as $x\to \infty$.  To \emph{prove} that $\lim_{x\to\infty}1/x = 0$, we
apply the definition to $f(x) = 1/x$, $L=0$.

For a given $\varepsilon>0$, we need to show that
\begin{equation}\label{eqn:1overx-small-for-x-large}
  \left|\frac1x - L\right|<\varepsilon \text{ for all } x>A
\end{equation}
provided we choose the right $A$.
  
How do we choose $A$?  $A$ is not allowed to depend on $x$, but it may
depend on $\varepsilon$.

Let's decide that we will always take $A>0$, so that we only need consider
positive values of $x$.  Then \eqref{eqn:1overx-small-for-x-large} simplifies to
\[
\frac 1x<\varepsilon
\]
which is equivalent to
\[
x>\frac1\varepsilon.
\]
This tells us how to choose $A$.  Given any positive $\varepsilon$, we will
simply choose
\[
  A= \text{ the larger of } 0 \text{ and } \frac1\varepsilon
\]
Then we have $|\frac1x-0| = \frac1x <\varepsilon$ for all $x>A$, so we
have proved that $\lim_{x\to\infty}1/x=0$.
\end{eg}

The properties of the limit given in section \ref{sec:properties-of-the-limit},
p.~\pageref{sec:properties-of-the-limit}, also apply to limits at infinity.
As with limits at finite $x$, it is usually more convenient to calculate limits
by using these properties than by direct application of the definition.

\begin{eg}{A rational function}\label{eg:rational-function-at-infinity}
A rational function is the quotient of two polynomials:\index{rational function}
\begin{equation}
  R(x) = \frac{a_nx^n+\cdots +a_1x+a_0}{b_mx^m+\cdots+b_1x+b_0}.
\end{equation}
The following trick allows us to evaluate the limit of any such function at infinity.

For example, let's compute
\[
\lim_{x\to\infty}\frac{3x^2+3}{5x^2+7x-39}.
\]
The trick is to factor $x^2$ from top and bottom. You get
\begin{align*}
  \lim_{x\to\infty}\frac{3x^2+3}{5x^2+7x-39}
  &= \lim_{x\to\infty}\frac{x^2}{x^2} \; \frac{3+3/x^2}{5+7/x-39/x^2}
  & \text{(algebra)}\\
  &= \frac{\lim_{x\to\infty}(3+3/x^2)}{\lim_{x\to\infty}(5+7/x-39/x^2)}
  &\text{(limit properties)}\\
  &= \frac35.
\end{align*}
At the end of this computation, we used the limit properties
$(P_\ast)$ to break the limit down into simpler pieces like
$\lim_{x\to\infty}39/x^2$, which we can directly evaluate; for example, we have
\[
\lim_{x\to\infty} 39/x^2 =\lim_{x\to\infty} 39\cdot \left( \frac1x
\right)^2 = \left( \lim_{x\to\infty}39\right)\cdot \left(
  \lim_{x\to\infty}\frac1x \right)^2 = 39 \cdot 0^2 =0.
\]
  The other terms are similar.

\end{eg}

\begin{eg}{Another rational function}
Compute
\[
\lim_{x\to\infty} \frac {2x}{4x^3+5}.
\]
We apply the same trick as in example \ref{eg:rational-function-at-infinity}
and factor $x$ out of the numerator and $x^3$ out of the denominator.
This leads to
\begin{align*}
  \lim_{x\to\infty} \frac {2x}{4x^3+5}
  &=\lim_{x\to\infty} \Bigl(\frac{x}{x^3}\;\frac{2}{4+5/x^3}\Bigr)\\
  &=\lim_{x\to\infty} \Bigl(\frac{1}{x^2}\;\frac{2}{4+5/x^3}\Bigr)\\
  &=\lim_{x\to\infty} \Bigl(\frac{1}{x^2}\Bigr)\cdot \Bigl(\lim_{x\to\infty}\frac{2}{4+5/x^3}\Bigr)\\
  &=0\cdot \tfrac24\\
  &=0.
\end{align*}

\end{eg}

<% end_sec('limits-at-infinity') %>

<% begin_sec("Limits that equal infinity",nil,'limit-equals-infinity') %>\index{limit!that equals infinity}
Figure \figref{telephone-wire} shows a telephone wire strung between two poles, which sags
by some amount $h$ in the middle. By increasing the tension $T$ in the wire, we can reduce
the sag. That is, the necessary tension $T$ is some function $T(h)$. There is a story, almost certainly
apocryphal, to the effect that a small-town mayor considered the sagging wires unsightly,
and instructed the public works department to tighten them up enough so that they wouldn't
sag at all.
<% marg(30) %>
<%
  fig(
    'telephone-wire',
    %q{A telephone wire sags by an amount $h$.}
  )  
%>
<% end_marg %>

It can be shown that the function $T(h)$ is approximately given by the equation
\begin{equation*}
  T = \frac{k}{h}\eqquad,
\end{equation*}
where $k$ is a constant.\footnote{The value of $k$ is $WL/8$, where $W$ is the weight of the
wire and $L$ is the horizontal length. The approximation is good if $h$ is small compared to $L$.}
When I ask students what happens to this equation when we plug in $h=0$, I always get
a chorus of ``undefined!'' This shows good mathematical training --- division by zero is
indeed undefined --- but doesn't give any real insight into what will go wrong when the
workers try to carry out the mayor's plan. If we make $h$ smaller and smaller $T$ will
get bigger and bigger. By making $h$ sufficiently small, we can make $T$ arbitrarily large.
The important insight here is that a quantity like $1/0$ isn't just undefined, it's undefined
because it's infinity, and infinity isn't a real number. If the workers actually
try to make $h=0$, they will simply have to tighten the wires so much that the wires break.

Another way of putting this is that the limit $\lim_{h\rightarrow0} T(h)$ fails to exist.
Although it's true that the limit doesn't exist, we can be more descriptive about the
reason that it doesn't. It's a limit that doesn't exist because it equals infinity.

Consider the limit
\[
\lim_{x\to 0} \frac{1} {x}.
\]
As $x$ decreases to $x=0$ through smaller and smaller positive values,
its reciprocal $1/x$ becomes larger and larger.  We say that instead
of going to some finite number, the quantity $1/x$ ``goes to
infinity'' as $x\searrow0$.  In symbols:
\begin{equation}
  \lim_{x\searrow 0} \frac{1} {x} = \infty.
  \label{eqn:03limit-invx-is-infty}
\end{equation}%
%\marginpar{\input ../figures/221/03inverse-of-x.tex }%
Likewise, as $x$ approaches $0$ through negative numbers, its
reciprocal $1/x$ drops lower and lower, and we say that $1/x$ ``goes
to $-\infty$'' as $x\nearrow 0$.  Symbolically,
\begin{equation}
  \label{eqn:03limit-invx-is-neg-infty}
    \lim_{x\nearrow 0} \frac{1} {x} = -\infty.
\end{equation}
The limits \eqref{eqn:03limit-invx-is-infty} and
\eqref{eqn:03limit-invx-is-neg-infty} are not like the normal limits we
have been dealing with so far.  Namely, when we write something like
\[
\lim_{x\to2} x^2 = 4
\]
we mean that the limit actually exists and that it is equal to $4$.
On the other hand, since we have agreed that $\infty$
is not a number (see p.~\pageref{infinity-is-not-a-hyperreal}),
the meaning of \eqref{eqn:03limit-invx-is-infty} cannot be to say that
``the limit exists and its value is $\infty$.''

%%graph%% limit-of-one-over-x-raw func=1/x format=svg xlo=0.1 xhi=10 ylo=0 yhi=10 with=lines samples=300
<% marg(300) %>
<%
  fig(
    'limit-of-one-over-x',
    %q{The function $1/x$ behaves badly near $x=0$.}
  )  
%>
<% end_marg %>

Instead, when we write
\begin{equation}
  \lim_{x\to a} f(x) = \infty
  \label{eqn:03limit-of-f-infty}
\end{equation}
for some function $y=f(x)$, we mean, \emph{by definition,} that the
limit of $f(x)$ does not exist, and that it fails to exist in a
specific way: as $x\to a$, the value of $f(x)$ becomes ``larger and
larger,'' and in fact eventually becomes larger than any finite
number.

The language in that last paragraph shows you that this is an
intuitive definition, at the same level as the first definition of
limit we gave in section \ref{subsec:limit-informal}, p.~\pageref{subsec:limit-informal}.  It contains the   
usual suspect phrases such as ``larger and larger,'' or ``finite
number'' (as if there were any other kind.)  A more precise definition
involving epsilons can be given, but in this course we will not go
into this much detail.

When a function is going to blow up at a certain point, there are two common behaviors. The first is
the one shown in figure \figref{limit-of-one-over-x} for $1/x$, where the limit is $+\infty$ on one
side and $-\infty$ on the other. If a limit is to be more than a one-sided limit, we want it to have
the same value on the left and right. In this example that doesn't happen, so only the one-sided limits
can be described as being positive- or negative-infinite:
\begin{align*}
  & \lim_{x\searrow0} \frac{1}{x} = +\infty \\
  & \lim_{x\nearrow0}  \frac{1}{x} = -\infty \\
  & \lim_{x\rightarrow0}  \frac{1}{x} \text{ can't be described as $+\infty$ or $-\infty$} 
\end{align*}
The function $1/x^2$, figure \figref{limit-of-one-over-x-squared}, exhibits the other
frequently encountered behavior. Here we have a positive blowup on both sides, so it isn't
just the one-sided limits that can be described.
\begin{align*}
  & \lim_{x\searrow0} \frac{1}{x^2} = +\infty \\
  & \lim_{x\nearrow0}  \frac{1}{x^2} = +\infty \\
  & \lim_{x\rightarrow0}  \frac{1}{x^2} = +\infty
\end{align*}


As a final comment on infinite limits, it is
important to realize that \eqref{eqn:03limit-of-f-infty} is not
a normal limit, and \emph{you cannot apply the limit rules to infinite
  limits. }  Here is an example of what goes wrong if you try anyway.

%%graph%% limit-of-one-over-x-squared-raw func=1/(x*x) format=svg xlo=0.1 xhi=10 ylo=0 yhi=10 with=lines samples=300
<% marg(300) %>
<%
  fig(
    'limit-of-one-over-x-squared',
    %q{The function $1/x^2$ blows up near $x=0$, but in a different way than $1/x$; it approaches
        \emph{positive} infinity on both sides.}
  )  
%>
<% end_marg %>

\begin{eg}{Trouble with infinite limits}
If you apply the limit properties to $\lim_{x\searrow0} 1/x = \infty$,
then you could conclude
\[
1 = \lim_{x\searrow 0} x\cdot \frac{1} {x}
 = \lim_{x\searrow 0} x \times \lim_{x\searrow 0}  \frac{1} {x}
 = 0 \times \infty
 = 0,
\]
because ``anything multiplied with zero is zero.''

After using the limit properties in combination with this infinite
limit we reach the absurd conclusion that $1=0$.  The moral of this
story is that you can't use the limit properties when some of the
limits are infinite.
\end{eg}


<% end_sec('limit-equals-infinity') %>

<% end_sec('limit-variations') %>

<% begin_sec("Curve sketching",nil,'curve-sketching') %>\index{curve sketching}
<% begin_sec("Sketching a graph without knowing its equation",nil,'sketch-without-equation') %>
The concepts of calculus, such as derivatives, limits, curvature, and concavity, can
guide us in analyzing the behavior of a function even when we don't know a formula for
the function. In economics, for example, these concepts are used heavily even though
real-world data can essentially never be described by a formula. This subsection
presents four examples in which we can use these concepts to sketch a function based on
our understanding of how the function should behave in real life.

<% begin_sec("The time to pay off a loan",nil,'loan-period') %>
Most people will end up borrowing money at some point in their lives, whether it's
credit card debt, a mortgage, a loan to buy a car, or a cash advance from a payday loan company.
One of the warning signs that you may be walking into an exploitative situation is if the
person trying to sell you the loan emphasizes the low monthly payment. Suppose that
you're borrowing \$10,000 to buy a car, and the monthly interest rate is 1\%.
Let $p$ be the monthly payment, and $T$ the time required in order to pay off
the loan. To understand what's going on here, you want to be able to
\emph{visualize} the graph of $T$ as a function of $p$. One fairly tedious way to do this would
be to find the equation of the function,
take a piece of graph paper and plot points. Another method would be to use
an expensive graphing calculator. But your knowledge of calculus gives you a method
that provides more insight with less work.

Clearly the smaller the payment, the longer it will take to pay off the loan.
This tells us that $T(p)$ is a \emph{decreasing} function; its derivative will always
be negative.

If $p$ is large, then you will pay off the loan so quickly that no significant amount
of interest accrues. Therefore at large values of $p$, we will have $T\approx(\$10,000)/p$.
This tells us that $\lim_{p\rightarrow\infty} T=0$. The graph of $T$ will approach the
horizontal axis more and more closely as $p$ gets bigger and bigger. We say that
the function $T(p)$ has a \emph{horizontal asymptote} at zero.
<% marg(300) %>
<%
  fig(
    'loan-period',
    %q{The time required to pay off a loan, as a function of the monthly payment.}
  )  
%>
<% end_marg %>

Finally, what happens if $p$ is small? Remember, interest on the loan is accruing
at a rate of 1\% monthly, or \$100 every month. It may sound like a good deal if
you're offered this loan with a low monthly payment of \$101, but if you take the
loan and always make the minimum payment, then the principal on the loan will only
go down by \$1 every month. You will die of old age before you pay off the car.
We can therefore tell that $\lim_{p\searrow\$100} T=\infty$. This is a
\emph{vertical asymptote} on the graph.
<% end_sec('loan-period') %>

Figure \figref{loan-period} shows what the graph must look like.

<% begin_sec("The Laffer curve",nil,'laffer-curve') %>
This example, a famous one, also has to do with money. In 1974, economist Arthur Laffer
presented the following argument about taxes to politicians Dick Cheney and Donald Rumsfeld,
sketching the resulting graph on a paper napkin. Consider the government's
tax revenue as a function of the tax rate. Clearly if the tax rate is zero, the government
gets zero revenue. Most people would assume that the function was a purely increasing one,
since raising the tax rate would always garner the government more money.

<% marg(-300) %>
<%
  fig(
    'laffer-curve',
    %q{The Laffer curve.}
  )  
%>
<% end_marg %>

But, Laffer
said, that isn't so. Imagine that the tax rate was 100\%, so that the government confiscated
all of everyone's earnings. Nobody would have any incentive to work, so they would stop
working, they would earn no taxable income, and revenue would drop to zero.
Laffer sketched a graph like figure \figref{laffer-curve} on a paper napkin for
Cheney and Rumsfeld. There should be some intermediate tax rate, he told them,
that would produce the maximum revenue. Later, when Ronald Reagan became president,
he cut taxes on the theory that the US was already on the right-hand side of the ``Laffer curve,''
so that, counterintuitively, the lower taxes would produce \emph{higher} revenue.
The results were not as Laffer had promised; the average annual budget deficit during the Reagan
administration was \$240 billion, compared to \$57 billion during the preceding Carter administration.

In calculus terms, our analysis of this function is an example of a result called Rolle's
theorem, p.~\pageref{subsec:rolle}. The idea is that if the function is smooth, then we expect its derivative to be
continuous. If the derivative is positive on the left and negative on the right, then it
must be zero at some intermediate point. This would be the point at which the function was
maximized.
<% end_sec('laffer-curve') %>

<% begin_sec("Skydiving",nil,'skydiving') %>
Figure \figref{skydiving} shows a skydiver's altitude as a function of time.
Early in the motion, soon after the person jumps out of the plane, the only significant
force is gravity, and the person falls with constant acceleration (section \ref{subsec:velocity},
p.~\pageref{subsec:velocity}). The drop relative to the initial position
equals $(1/2)at^2$, which is the equation of a parabola. 
<% marg(300) %>
<%
  fig(
    'skydiving',
    %q{Altitude as a function of time for a skydiver.}
  )  
%>
<% end_marg %>

<% marg(-300) %>
<%
  fig(
    'anchor',
    %q{A rock-climbing anchor.}
  )  
%>
<% end_marg %>

But as the downward (negative) velocity increases,
the upward force of air friction gets stronger and stronger. In the opposite
limit of $t\rightarrow\infty$, the force of air friction gets closer and closer to
being strong enough to cancel the force of gravity. In this limit, Newton's
second law (section \ref{subsec:newton-second-law}, p.~\pageref{subsec:newton-second-law})
predicts an acceleration of zero. An acceleration of zero corresponds to constant
velocity, so that the graph asymptotically approaches a line whose slope is the velocity.

This graph demonstrates two mathematical properties. It has a \emph{y-intercept},
which is the initial altitude. It also has an oblique asymptote, i.e., an asymptotic line
that is neither horizontal nor vertical.
<% end_sec('skydiving') %>

<% begin_sec("A rock-climbing anchor",nil,'anchor') %>
For safety, rock climbers and mountaineers often
wear a climbing harness and tie in to other climbers on a rope
team or to anchors such as pitons or snow anchors. When using anchors, the climber usually wants to be
protected by
more than one, both for extra strength and for redundancy in case one fails.
Figure \figref{anchor} shows such an arrangement, with the climber hanging from a pair of anchors forming a ``Y''
at an angle $\theta$.  The usual advice is to make $\theta<90\degunit$; for large values of $\theta$,
the stress placed on the anchors can be many times greater than the actual load $L$,
so that two anchors are actually \emph{less} safe than one.

Consider the stress on the anchor $S$ as a function of $\theta$. For physical
reasons similar to those discussed in the example of the telephone wire
(section \ref{subsec:limit-equals-infinity}, p.~\pageref{subsec:limit-equals-infinity}),
$S$ must approach infinity as $\theta$ approaches 180 degrees; no matter how tight
the anchor strands are made, the carabiner (hook) at the center will never be pulled up
quite as high as the anchors.

At $\theta=0$, we can see that each anchor strand will support half the load. The
\emph{y-intercept} of the graph equals $L/2$.

We can gain further insight by extending the range of possible values for $\theta$ to
include negative angles. Physically, this corresponds to bringing the anchor strands
past one another and swapping the roles of the two anchors. Since the physical setup is
symmetrical, the function $S(\theta)$ must have the property $S(\theta)=S(-\theta)$,
i.e., it is an \emph{even} function. It might seem pointless to discuss this symmetry,
but it tells us something important. An argument identical to the one in
section \ref{subsec:derivative-of-x-squared}, p.~\pageref{subsec:derivative-of-x-squared}, tells
us that based on this symmetry, the derivative $S'$ must equal zero at $\theta=0$. This means
that for small values of $\theta$, the strain on the anchor will be very nearly the same as
for $\theta=0$, i.e., hardly any greater than half the load. Thus any small value of $\theta$
is about equally good, but very large values could be a deadly mistake.
<% end_sec('anchor') %>


<% end_sec('sketch-without-equation') %>
<% begin_sec("Sketching $f'$ and $f''$ given the graph of $f$",nil,'sketch-derivatives') %>
In figure \figref{beer-sketching} we revisit the example of fermenting beer
(section \ref{sec:changing-change}, p.~\pageref{sec:changing-change}). (Feel free to mark
your place in the book and make a trip to the fridge before continuing.)
The top panel of the graph would probably have been the easiest to sketch starting from
scratch. Clearly the amount of $\zu{CO}_2$ produced starts off at zero, it rises, and it must
eventually flatten out and approach a horizontal asymptote, since the yeast use up all their
food and can't produce any more. This kind of vaguely S-shaped curve is in fact encountered
in many situations, and is often referred to as a ``yeast curve.''
<% marg(40) %>
<%
  fig(
    'beer-sketching',
    %q{Sketching $y'$ and $y''$ given the graph of $y$.}
  )  
%>
<% end_marg %>

Now suppose we know $y$ and we want to find $y'$ and $y''$. The basic concept is that the \emph{slope}
of each graph in the stack gives the \emph{value} of the graph below it. The slope of the tangent line
to the $y$ graph
at time A is small and positive, while the slope at B is larger and positive. Therefore the
\emph{values} of $y'$ at these times must be small and positive, then larger and positive. At time
C, the slope of the $y$ graph is as great as it will ever be. Therefore the $y'$ graph has
a maximum there. The slope of $y$ gets smaller at D and still smaller at E, so the value of $y'$
must taper off correspondingly.

Now that we've sketched the graph of $y'$, we can continue the process and construct \emph{its}
derivative, $y''$. At time C the slope of the $y'$ graph is zero, so the value of the $y''$ graph
is zero; this is a point of inflection. At times earlier than C the slope of $y'$ is positive,
while at times later than C it's negative. Therefore we must have $y''>0$ before C and
$y''<0$ after.

We can also relate the properties of the $y''$ graph directly to those of the $y$ graph.
The second derivative is a measure of curvature, and its sign indicates concavity.
The $y$ graph is concave up before C and concave down after. This matches up with the
signs of $y''$.

\startdq

\begin{dq}\label{dq:inconsistent-graphs}
Figure \figref{dq-inconsistent-graphs} shows three stacks of graphs, each of which
is supposed to represent the position, velocity, and acceleration of an object.
Explain how each set of graphs contains inconsistencies, and fix them.
\end{dq}

<% 
  fig(
    'dq-inconsistent-graphs',
    'Discussion question \ref{dq:inconsistent-graphs}.',
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>

<% end_sec('sketch-derivatives') %>
<% begin_sec("Sketching a graph given its equation",nil,'sketch-given-equation') %>
If we have an equation defining a function, then the following procedure is
often a fairly efficient way of sketching its graph. Often we are especially
interested in finding the function's local maxima and minima, including the \emph{absolute}
or \emph{global} maxima and minima. That is, the absolute maximum is the greatest value ever attained
by the function, and similarly for the absolute minimum.

\begin{enumerate}
\item Find all solutions of $f'(x)=0$ in the interval $[a,b]$: these are called
  the \textit{critical} or \textit{stationary} points for $f$.
\item Find the sign of $f'(x)$ at all other points.
\item Each stationary point at which $f'(x)$ actually changes sign is a local
  maximum or local minimum.  Compute $f(x)$ at each
  stationary point.
\item Compute the  values of the function $f(a)$ and $f(b)$ at the endpoints of the interval.
\item The absolute maximum is attained at the stationary point or the boundary
  point with the highest value of $f$; the absolute minimum occurs at the
  boundary or stationary point with the smallest value.
\end{enumerate}
If the interval is unbounded, then instead of computing the values $f(a)$ or
$f(b)$,  you should instead compute $\lim_{x\to\pm\infty}f(x)$.

As an example, let's sketch the graph of the rational function
\[
f(x) = \frac{x(3-4x)}{1+x^2}\eqquad.
\]
By looking at the signs of numerator and denominator we see that
\begin{center}
  $f(x)>0$ for $0<x<\frac{3}{4}$\\[2pt]
  $f(x)<0$ for $x<0$ and also for $x>\frac{3}{4}$.
\end{center}
We compute the derivative of $f$:
\[
f'(x) = \frac{-3x^2-8x+3}{\left(1+x^2\right)^2}.
\]
Hence $f'(x) = 0$  if and only if
\[
-3x^2-8x+3 = 0\eqquad,
\]
and the solutions to this quadratic equation are $-3$ and $1/3$.
% Checked:
%   calc -e "d=.001; x=-3; f1=[x(3-4x)]/(1+x^2); x=x+d; f2=[x(3-4x)]/(1+x^2); (f2-f1)/d"
%   calc -e "d=.001; x=1/3; f1=[x(3-4x)]/(1+x^2); x=x+d; f2=[x(3-4x)]/(1+x^2); (f2-f1)/d"
These two   
roots will appear several times, and it will shorten our formulas if we
abbreviate
\[
  A= -3 \text{ and } B= 1/3\eqquad.
\]

To see if the derivative changes sign we factor the numerator and denominator.
The denominator is always positive, and the numerator is
\[
-3x^2-8x+3 = -3\left(x^2+\frac{8}{3}x-1\right) =-3(x-A)(x-B)\eqquad.
\]
Therefore
\[
f'(x)
\begin{cases}
  <0 & \text{for }x<A \\
  >0 &\text{for } A<x<B \\
  <0 &\text{for } x>B
\end{cases}
\]
It follows that $f$ is decreasing on the interval $(-\infty, A)$, increasing on
the interval $(A, B)$ and decreasing again on the interval $(B, \infty)$
(figure \figref{curve-sketching-signs-of-derivative}).
Therefore
\begin{center}
  $A$ is a local minimum, and $B$ is a local maximum.
\end{center}
Are these global maxima and minima?

<% marg(0) %>
<%
  fig(
    'curve-sketching-signs-of-derivative',
    %q{The sign of the derivative changes at A and B.}
  )  
%>
<% end_marg %>

Since we are dealing with an unbounded interval we must compute the limits of
$f(x)$ as $x\to\pm\infty$.  We find
\[
\lim_{x\to\infty} f(x) = \lim_{x\to-\infty} f(x) = -4.
\]
Since $f$ is decreasing between $-\infty$ and $A$, it follows that
\[
f(A) \leq f(x) < - 4 \text{ for } -\infty<x\leq A.
\]
Similarly, $f$ is decreasing from $B$ to $+\infty$, so
\[
-4 < f(x) \leq f(B) \text{ for } B < x<\infty.
\]
Between the two stationary points the function is increasing, so
\[
f(A) \leq f(x) \leq f(B) \text{ for } A\leq x\leq B.
\]
From this it follows that $f(x)$ has a global minimum when $x=A=-3$
and has a global maximum when $x=B=1/3$.

%%graph%% curve-sketching-rational-raw func=(x*(3-4*x))/(1+x**2) format=svg xlo=-9 xhi=11 ylo=-5 yhi=1 with=lines samples=1000

<%
  fig(
    'curve-sketching-rational',
    %q{The graph of $f(x)=x(3-4x)/(1+x^2)$.},
    {
      'width'=>'wide',
      'sidecaption'=>true
    }
  )
%>


<% end_sec('sketch-given-equation') %>
<% end_sec('curve-sketching') %>

<% begin_sec("Completeness",nil,'completeness') %>\index{real numbers!completeness property}
<% begin_sec("The completeness axiom of the real numbers",nil,'completeness-axiom') %>
Calculus is the study of rates of change (differentiation) and how change
accumulates (integration, which we haven't encountered yet).
What changes is always a \emph{function}, and the function takes an input
value that belongs to its domain and gives back an output that belongs to its
range. The domain and range could in principle be sets of integers, rational
numbers, real numbers, complex numbers, or hyperreal numbers
(section \ref{sec:safe-handling-of-dx}, p.~\pageref{sec:safe-handling-of-dx}).
These number systems all share many of the same properties, but 
just as the ocean is the natural setting for a pirate story,
there is a sense in which the real numbers are the natural setting in which to
do calculus. Throughout this book, without specifically commenting on it so far,
we've been considering only functions that take real-number inputs and give
back real-number outputs: real functions.


<% marg(0) %>
<%
  fig(
    'cannonball-hits-water',
    %q{A cannonball is fired horizontally, and hits the water at $y=0$.}
  )  
%>
<% end_marg %>

%%graph%% cannonball-hits-water func=2-x**2 format=eps xlo=0.0 xhi=2 ylo=-1 yhi=2 with=lines samples=100

What's so special about real functions?
We can define functions whose inputs and outputs are, say, integers, and
such functions are of interest in many fields of mathematics. But real
functions are especially well suited to describing rates of change.
As an example, the graph in figure \figref{cannonball-hits-water} shows the function $f(x)=2-x^2$.
Let's say this represents the arc of a cannon-ball shot off of a cliff
into the ocean, where a $y$ coordinate of $0$ represents the surface of the water.
Our geometrical intuition tells us that if the ball starts above the water,
and later on ends up below it, then there must be
some point at which it enters the water. In other words, if the graph
of the function
$f$ cuts across the line $y=0$, then there must be a point at which they
coincide.

But if we consider a set of numbers more restricted than the   
real numbers, this may not happen. For example, suppose we take $f$ to be
a function whose inputs and outputs are rational
numbers. Recall that a rational number is any number that can be expressed
as an integer divided by another integer, e.g., the fraction 2/3. But the
place where our cannonball crosses sea level has $x=\sqrt{2}$, which is 
not a rational number. This example shows that the graphs of two rational-number
functions can cut across one another without ever touching! This offends
our intuition about rates of change, since we expect that if we change
a variable smoothly from one value to another, it should visit every value
in between.

What is the special ingredient, the secret sauce that allows the real number
system to avoid such paradoxical results as the one about the cannonball?
It seems that the reals are somehow more \emph{densely packed} on the number line than the rationals, but how
do we define this density property in mathematical terms? It can't be any
of the elementary properties of the reals
(section \ref{sec:elementary-reals}, p.~\pageref{sec:elementary-reals}),
since the rationals also satisfy all of those properties. We need to add a new
axiom, which is called the completeness axiom.

One possible way of stating such an axiom
is the following. 

\begin{important}[Completeness axiom]
Let P and Q be sets of  numbers such that every number in P is smaller than every number in Q.
Then there exists some  number $z$ such that $z$ is greater than or equal to every number in P,
but less than or equal to any number in Q.
\end{important}\index{completeness|see{real numbers}}

<% marg(0) %>
<%
  fig(
    'cannonball-completeness',
    %q{1.~The sets P and Q are separated on the number line so that every point in P is to the left of every point in Q.
          By the completeness axiom, a number like $z$ exists.
       2.~By the completeness axiom, the curve $f(x)=2-x^2$ must intersect the axis. The point of intersection is
          $z=\sqrt{2}$. The completeness axiom doesn't hold for the rational numbers, and we can see that here because
          $z$ is an irrational number.}
  )  
%>
<% end_marg %>

As an example, let P be the set of all numbers $x$ such that $x^2<2$,
and Q the set of $x$ such that $x^2\ge2$.
Then the number $z$ would have to be $\sqrt{2}$,
which shows that the rationals are not complete. The reals are complete, and the completeness
axiom can serve as one of the fundamental axioms of the real numbers.

\enlargethispage{-5\baselineskip}

\pagebreak

\enlargethispage{-5\baselineskip}

The completeness axiom is of a fundamentally different character than the elementary axioms.
The elementary axioms make statements such as ``for any number $x$, \ldots'' or
``for any numbers $x$ and $y$, \ldots'' The completeness axiom says ``for any \emph{sets}
of numbers P and Q, \ldots''

\vfill

\begin{eg}{Every decimal is a real number}
Consider the infinite decimal
\begin{equation*}
  3.141592\ldots\eqquad,
\end{equation*}
which is the decimal expansion of $\pi$. We can use the completeness axiom to prove that this is
a real number. Let P be the list of rational numbers given by $\{3,\ 3.1,\ 3.14,\ 3.141,\ \ldots\}$.
Let Q be the set of rational numbers that are larger than every number in P. Then the real
number whose existence is asserted by the completeness axiom is exactly $\pi$. Similar reasoning
shows that any decimal corresponds to some real number (which can be shown to be unique).
(Note, however, that the same
real number can have more than one decimal expansion. For example, the infinite repeating
decimals $1.000\ldots$ and $0.999\ldots$ both equal 1.)
\end{eg}

\vfill

\begin{eg}{The Archimedean property}\label{eg:archimedean}\index{real numbers!Archimedean property}
The Archimedean principle states that there is no positive real number that is less than $1/1$, less than
$1/(1+1)$, less than $1/(1+1+1)$, and so on.\footnote{Cf. section \ref{sec:safe-handling-of-dx},
p.~\pageref{sec:safe-handling-of-dx}. For an application 
to economics, see rule 3, p.~\pageref{vnm-axioms}.} In other words, it says that there are no real numbers
that are infinitely small, but still greater than zero. The Archimedean property can be proved
from the completeness property. For suppose, to the contrary, that we did have such a real number.
Then it would be less than $1/10$, so its first decimal place would be 0. It would also be less
than $1/100$, so its second decimal place would also be zero. Continuing in this way, we find that
the decimal expansion of such a number must be $0.000\ldots$, with the zeroes repeating forever.
But this is the decimal expansion of zero, and we already know that every decimal expansion corresponds
to a unique real number. Therefore our number is zero, and this is a contradiction, since we
assumed that it violated the Archimedean principle, which refers to a \emph{positive} real number.
\end{eg}

<% end_sec('completeness-axiom') %>
<% begin_sec("The intermediate and extreme value theorems",4,'intermediate-and-extreme') %>
The following two theorems can be proved from the completeness property and
the elementary properties of the reals, but we will not give the proofs here.

<% begin_sec("The intermediate value theorem",nil,'intermediate-value-theorem') %>
Intuitively, the intermediate value theorem\index{intermediate value theorem}
says that the real numbers aren't susceptible to
paradoxes like the cannonball paradox described above. Or, we can say that
if you are moving continuously along a road, 
and you get from point A to point B, then you must also visit every other point
along the road; only by teleporting (by moving discontinuously) could you
avoid doing so. More formally, the theorem says this:
<% marg(0) %>
<%
  fig(
    'intermediate-value-theorem',
    %q{The intermediate value theorem states that if the function is continuous, it must pass through $y_3$.}
  )  
%>
<% end_marg %>

\begin{theorem}[Intermediate value theorem]
If $y$ is a continuous real-valued function on the real interval from $a$ to $b$,
and if $y$ takes on values $y_1$ and $y_2$ at certain points within this interval, then for any $y_3$ between $y_1$ and
$y_2$, there is some real $x$ in the interval for which $y(x)=y_3$.
\end{theorem}

\begin{eg}{}
\egquestion Show that there is a solution to the equation $10^x+x=1000$.

\eganswer We expect there to be a solution near $x=3$, where the function $f(x)=10^x+x=1003$ is just a little too big.
On the other hand, $f(2)=102$ is much too small. Since $f$ has values above and below 1000 on the interval from
2 to 3, and $f$ is continuous, the intermediate value theorem proves that a solution exists between 2 and 3.
If we wanted to find a better numerical approximation to the solution, we could do it using Newton's
method, which is introduced in section \ref{sec:newtons-method}.
\end{eg}

%%graph%% x-minus-cos-x func=(x-cos(x)) format=eps xlo=-8 xhi=8 ylo=-8 yhi=8 xtic_spacing=4 ytic_spacing=4

<% marg(-100) %>
<%
  fig(
    'x-minus-cos-x',
    %q{The function $x-\cos x$ constructed in example \ref{eg:x-minus-cos-x}.}
  )  
%>
<% end_marg %>

\begin{eg}{}\label{eg:x-minus-cos-x}
\egquestion Show that there is at least one solution to the equation $\cos x=x$, and give bounds on its location.

\eganswer This is what's known as
a transcendental equation, and no amount of fiddling with algebra and trig identities will
ever give a closed-form solution, i.e., one that can be written down with a finite number of arithmetic
operations to give an exact result. However, we can easily prove that at least one solution exists, by
applying the intermediate value theorem to the function $f(x)=x-\cos x$. The cosine function is bounded between
$-1$ and 1, so $f$ must be negative for $x<-1$ and positive for $x>1$. By the intermediate value
theorem, there must be a solution in the interval $-1 \le x \le 1$. The graph, \figref{x-minus-cos-x}, verifies
this, and shows that there is only one solution.
\end{eg}


\pagebreak

\begin{eg}{Supply and demand}\label{eg:supply-and-demand}\index{supply and demand}\index{market equilibrium}
Figure \figref{eg-supply-and-demand} shows two graphs representing the supply and demand of some good
on a free market. The function $D(p)$ shows the quantity that buyers would willingly buy at unit price
$p$. Normally $D$ is a decreasing
function: if the price goes up, people don't buy as much. (But cf.~problem \ref{hw:perfectly-inelastic-demand},
p.~\pageref{hw:perfectly-inelastic-demand}.) The function $S(p)$ shows the quantity that the seller
would willingly offer if the unit price was $p$. Often $S$ is an increasing function.
For example, Boeing might only be able to produce more passenger jets by paying their
workers overtime, which would create a cost that they would pass on to their customers.

Suppose that, as in the example shown in the figure, $D$ starts out higher than $S$ on the left, but ends
up lower than $S$ on the right. Then we expect geometrically that if the curves are continuous,
they must cross at some point. This can be proved using the same technique as in example
\ref{eg:x-minus-cos-x}. We construct a function $f(p)=S(p)-D(p)$, which goes from negative to positive.
By the intermediate value theorem, there must be some point where $f=0$, meaning that $S=D$. This
crossing point is the free-market equilibrium.

The intermediate value theorem holds for real numbers, but in fact neither the price nor the quantity
is free to have any real-number value. For example, Boeing can't sell half an airplane.
In some cases this might mean that the free-market equilibrium defined by $S=D$ would not exist.
An example might be the Concorde, a supersonic passenger jet, which flew from 1969 to 2003.
The nonexistence of the market for this plane today may indicate that the supply and demand curves
now cross at a quantity that is greater than 0 and less than 1,
which is not a possible free-market equilibrium because the planes can only be sold in whole numbers.
\end{eg}


<% marg(300) %>
<%
  fig(
    'eg-supply-and-demand',
    %q{Example \ref{eg:supply-and-demand}.}
  )  
%>
<% end_marg %>

\begin{eg}{}
\egquestion Prove that every odd-order polynomial $P$ with real coefficients has at least one real root $x$, i.e., a
point at which $P(x)=0$.

\eganswer
Example \ref{eg:x-minus-cos-x} might have given the impression that there was nothing
to be learned from the intermediate value theorem that couldn't be determined by graphing,
but this example clearly can't be solved by graphing, because we're trying to prove
a general result for all polynomials.

To see that the restriction to odd orders is necessary, consider the polynomial $x^2+1$, which has no real roots
because $x^2>0$ for any real number $x$.

To fix our minds on a concrete example for the odd case, consider the polynomial $P(x)=x^3-x+17$.
For large values of $x$, the linear and constant terms will be negligible compared to the $x^3$ term,
and since $x^3$ is positive for large values of $x$ and negative for large negative ones, it follows
that $P$ is sometimes positive and sometimes negative. Therefore by the intermediate value theorem
$P$ has at least one root.

This argument didn't depend much on the specific polynomial $P$ chosen
as an example. The fact that $P$ was positive for large $x$ and negative for large negative $x$
followed merely from the fact that $P$ was of odd order. Therefore the result holds for all polynomials
of odd order.
\end{eg}

\begin{eg}{}\label{eg:x-minus-sin-1-over-x}
\egquestion Show that the equation $x=\sin 1/x$ has infinitely many solutions.

\eganswer  This is another example that can't be solved by graphing; there is
clearly no way to prove, just by looking at a graph like \figref{x-minus-sin-1-over-x}, that 
the function $f(x)=x-\sin 1/x$ crosses the
$x$ axis \emph{infinitely} many times. The graph does, however, help us to gain intuition for what's
going on. As $x$ gets smaller and smaller, $1/x$ blows up, and $\sin 1/x$ oscillates more and more
rapidly. The function $f$ is undefined at 0, but it's continuous everywhere else, so we can apply the
intermediate value theorem to any interval that doesn't include 0.

We want to prove that for any positive $u$, there exists an $x$ with $0<x<u$ for which $f(x)$ has
either desired sign. Let $n$ be an even integer such that $n>10$ and also $\pi n>1/u$. Then clearly
$f(x)$ is negative at $x=1/(\pi n+\pi/2)<u$, since $\sin 1/x=1$ and $x$ is small. Similarly,
$f(x)$ is positive at $x=1/(\pi n+3\pi/2)<u$. This establishes the desired result.
\end{eg}
<% marg(70) %>
<%
  fig(
    'x-minus-sin-1-over-x',
    %q{The function $x-\sin 1/x$.}
  )  
%>
<% end_marg %>

%%graph%% x-minus-sin-1-over-x func=(x-sin(1/x)) format=eps xlo=-2 xhi=2 ylo=-2 yhi=2 xtic_spacing=1 ytic_spacing=1


<% end_sec('intermediate-value-theorem') %>
<% begin_sec("The extreme value theorem",nil,'extreme-value-theorem') %>

We've seen that that locating maxima and minima of functions may in general
be fairly difficult, because there are so many different ways in which a function can attain an extremum:
e.g., at an endpoint, at a place where its derivative is zero, or at a nondifferentiable kink. The following
theorem allows us to make a very general statement about all these possible cases, assuming only continuity.

\begin{theorem}[Extreme value theorem]\label{extreme-value-theorem}
If $f$ is a continuous real-valued function on the real-number
interval defined by $a \le x \le b$, then $f$ has maximum and minimum values on that interval, which are
attained at specific points in the interval.\index{extreme value theorem}
\end{theorem}

Let's first see why the assumptions are necessary. If we weren't confined to a finite interval, then
$y=x$ would be a counterexample, because it's continuous and doesn't have any maximum or minimum value.
If we didn't assume continuity, then we could have a function defined as $y=x$ for $x < 1$, and $y=0$ for
$x \ge 1$; this function never gets bigger than 1, but it never attains a value of 1 for any specific value of $x$.
If we didn't assume a real function, then we could have, for example, the function $f(x)=(x^2-2)^2$ defined on 
the rational numbers, which would never attain the minimum value of 0 because $\sqrt{2}$ isn't
a rational number.

\begin{eg}
\egquestion Find the maximum value of the polynomial $P(x)=x^3+x^2+x+1$ for $-5 \le x \le 5$.

\eganswer Polynomials are continuous, so the extreme value theorem guarantees that such a
maximum exists. Suppose we try to find it by looking for a place where the derivative is zero.
The derivative is $3x^2+2x+1$, and setting it equal to zero gives a quadratic equation, but application of the
quadratic formula shows that it has no real solutions. It appears that the function doesn't have a maximum
anywhere (even outside the interval of interest) that looks like a smooth peak. Since it doesn't have kinks or
discontinuities, there is only one other type of maximum it could have, which is a maximum at one
of its endpoints. Plugging in the limits, we find $P(-5)=-104$ and $P(5)=156$, so we conclude that
the maximum value on this interval is 156.
\end{eg}
<% end_sec('extreme-value-theorem') %>

<% end_sec('intermediate-and-extreme') %>
<% begin_sec("Rolle's theorem and the mean-value theorem",nil,'rolle') %>
On p.~\pageref{subsubsec:laffer-curve}, in the example of the Laffer curve from economics, we
got a preview of the following intuitively appealing theorem.

<% marg(0) %>
<%
  fig(
    'rolle',
    %q{Rolle's theorem.}
  )  
%>
<% end_marg %>

\begin{theorem}[Rolle's theorem]\index{Rolle's theorem}
Let $f$ be a function that is continuous on the interval $[a,b]$ and differentiable on $(a,b)$,
and let $f(a)=f(b)$. There there exists a point $x\in(a,b)$ such that $f'(x)=0$.
\end{theorem}

Proof: By the extreme value theorem, $f$ attains its maximum and minimum values in $[a,b]$.
If both of these are at endpoints, then $f$ is a constant function, and the theorem holds
trivially. Suppose instead that at least one of these extrema is on the interior of the interval.
Then by the theorem given in section \ref{subsec:zero-deriv-at-extremum}, $f'$ is zero at that
point, and the theorem also holds.\myqed

Rolle's theorem can be straightforwardly generalized to the following.

<% marg(0) %>
<%
  fig(
    'mean-value',
    %q{The mean value theorem.}
  )  
%>
<% end_marg %>

\begin{theorem}[Mean value theorem]\index{mean value theorem}
Let $f$ be a function that is continuous on the interval $[a,b]$ and differentiable on $(a,b)$.
There there exists a point $x\in(a,b)$ such that
\begin{equation*}
  f'(x) = \frac{f(b)-f(a)}{b-a}\eqquad,
\end{equation*}
meaning that the derivative equals the average (mean) rate of change of the function between the
endpoints of the interval.
\end{theorem}

\noindent ``Mean'' is just a fancy word for ``average.'' In general, it's a mistake to try to calculate
a rate of change without calculus, using $\Delta y/\Delta x$, unless the rate of change is
constant. The mean value theorem says that just as a broken clock is right twice a day, there
is at least one point where $\Delta y/\Delta x$ gives the right answer.

Proof: Define the function
\begin{equation*}
  \ell(x)=a+\frac{f(b)-f(a)}{b-a}(x-a)\eqquad,
\end{equation*}
which is the point-slope form of the line passing through the endpoints of the graph of $f$.
Define a new function $g(x)=f(x)-\ell(x)$, so that $g(a)=g(b)=0$. Applying Rolle's theorem to $g$,
we find that there is some point where $f'(x)=\ell'(x)$, which is the desired result.\myqed
<% end_sec('rolle') %>
<% end_sec('completeness') %>
<% begin_sec("Two tricks with limits",nil,'limit-tricks') %>
<% begin_sec("Rational functions that give $0/0$",nil,'rational-zero-over-zero') %>
Suppose we want to compute the following limit:
\begin{equation*}
  \lim_{x\to 2} \frac{x^2-2x}{x^2-4}
\end{equation*}
We first use the limit
properties to find
\[
\lim_{x\to2} x^2-2x = 0 \text{ and }\lim_{x\to 2}x^2 - 4 = 0.
\]
Now to complete the computation we would like to apply the property
$(P_6)$ about quotients, but this would give us
\[
\lim_{x\to2} f(x) = \frac 00.
\]
The denominator is zero, so we were not allowed to use $(P_6)$ (and the
result doesn't mean anything anyway).  We have to do something else.

The function we are dealing with is a \emph{rational function}, which means, as mentioned
in example \ref{eg:rational-function-at-infinity}, p.~\pageref{eg:rational-function-at-infinity},
that it is the quotient of two polynomials.  For such functions there is an
algebra trick that always allows you to compute the limit even if you
first get $\frac 00$.  The thing to do is to divide numerator and
denominator by $x-2$.  In our case we have
\[
x^2-2x = (x-2)\cdot x, \qquad x^2-4 = (x-2)\cdot (x+2)
\]
so that
\[
\lim_{x\to2} f(x) =\lim_{x\to2} \frac{(x-2)\cdot x } {(x-2)\cdot (x+2)} =
\lim_{x\to2} \frac{x}{x+2}.
\]
After this simplification we \emph{can} use the properties $(P_{\ldots})$
to compute
\[
\lim_{x\to2} f(x) = \frac{2}{2+2} = \frac12.
\]

<% end_sec('rational-zero-over-zero') %>
<% begin_sec("The ``don't make $\\delta$ too big'' trick",nil,'dont-choose') %>
In this section we describe a trick, the ``don't make $\delta$ to too big'' trick,
that is sometimes helpful when we want to evaluate a limit directly from the epsilon-delta definition.
Say we want to prove that $\lim_{x\to 1}x^2 = 1$. This may not seem to require a fancy
proof, since obviously plugging in $x=1$ gives $x^2=1$. But since functions can be
discontinuous, plugging in does not always prove the value of a limit.
Also, this example will be an excuse to develop a technique that can be useful in
less trivial cases.

We have $f(x) = x^2$, $a=1$, $L=1$, and as usual when computing a limit the question is, ``how small
should $|x-1|$ be to guarantee $|x^2-1|<\varepsilon$?''

We begin by estimating the difference $|x^2-1|$
\[
|x^2-1| = |(x-1)(x+1)| = |x+1|\cdot|x-1|.
\]
As $x$ approaches 1 the factor $|x-1|$ becomes small, and if the other
factor $|x+1|$ were a constant (e.g.\ $2$ as in the previous example) then
we could find $\delta$ as before, by dividing $\varepsilon$ by that
constant.

Here is a trick that allows you to replace the factor $|x+1|$ with a
constant.  We hereby agree \textit{that we always choose our $\delta$ so
that $\delta\leq 1$.}  If we do that, then we will always have
\[
|x-1|<\delta\leq 1, \text{i.e. }|x-1|<1,
\]
and $x$ will always be between $0$ and $2$. Therefore
\[
|x^2-1| = |x+1|\cdot|x-1|<3|x-1|.
\]
If we now want to be sure that $|x^2-1|<\varepsilon$, then this calculation
shows that we should require $3|x-1|<\varepsilon$, i.e.\
$|x-1|<\frac13\varepsilon$.  So we should choose $\delta\leq
\frac13\varepsilon$.  We must also live up to our promise never to choose
$\delta>1$, so if we are handed an $\varepsilon$ for which
$\frac13\varepsilon>1$, then we choose $\delta=1$ instead of $\delta =
\frac13\varepsilon$.  To summarize, we are going to choose
\[
\delta = \text{the smaller of }1\text{ and }\frac13\varepsilon.
\]
We have shown that if you choose $\delta$ this way, then $|x-1|<\delta$
implies $|x^2-1|<\varepsilon$, no matter what $\varepsilon>0$ is.
                                                                                                                 
The expression ``the smaller of $a$ and $b$'' shows up often, and is
abbreviated to $\min(a, b)$.  We could therefore say that in this problem
we will choose $\delta$ to be
\[
\delta = \min \bigl(1, \tfrac13 \varepsilon\bigr).
\]

\begin{eg}{}
\egquestion Show that $\lim_{x\to 4}1/x = 1/4$.

\eganswer We apply the definition with $a=4$, $L=1/4$ and $f(x) = 1/x$.
Thus, for any $\varepsilon>0$ we try to show that if $|x-4|$ is small
enough then one has $|f(x)-1/4|<\varepsilon$.

We begin by estimating $|f(x)-\frac14|$ in terms of $|x-4|$:
\[
|f(x)-1/4| = \left|\frac1x-\frac14\right| = \left| \frac{4-x}{4x}\right| =
\frac{|x-4|}{|4x|} =\frac{1}{|4x|}\,|x-4|.
\]
As before, things would be easier if $1/|4x|$ were a constant.  To achieve
that we again agree not to take $\delta>1$.  If we always have $\delta\leq
1$, then we will always have $|x-4|<1$, and hence $3<x<5$.  How large can
$1/|4x|$ be in this situation?  Answer: the quantity $1/|4x|$ increases as
you decrease $x$, so if $3<x<5$ then it will never be larger than
$1/|4\cdot 3| = \frac1{12}$.

We see that if we never choose $\delta>1$, we will always have
\[
|f(x) - \tfrac14|\leq \tfrac1{12}|x-4| \quad\text{for}\quad |x-4|<\delta.
\]
To guarantee that $|f(x)-\frac14|<\varepsilon$ we could therefore require
\[
\tfrac1{12} |x-4|<\varepsilon, \quad\text{i.e.}\quad |x-4| <12\varepsilon.
\]
Hence if we choose $\delta=12\varepsilon$ or any smaller number, then
$|x-4|<\delta$ implies $|f(x)-4|<\varepsilon$.  Of course we have to honor
our agreement never to choose $\delta>1$, so our choice of $\delta$ is
\[
\delta = \text{the smaller of }1\text{ and }12\varepsilon = \min \bigl(1,
12\varepsilon\bigr).
\]
\end{eg}

<% end_sec('dont-choose') %>
<% end_sec('limit-tricks') %>


<% begin_hw_sec %>

<% hw('difference-of-roots-at-infty',{'solution'=>true}) %>
<% hw('parallel-resistance',{'solution'=>true}) %>

  <% hw_block(2) %>

<% hw('limits-of-e-to-minus-1-over-x',{'solution'=>true}) %>
% most of the following are from Robbin, p. 50
<% hw('big-powers') %>
<% hw('limits-at-infinity') %>
<% hw('infinity-from-the-northeast') %>
<% hw('no-limit-from-above') %>

% robbin, p. 51
<% hw('spline-1') %>
<% hw('spline-2') %>

<% hw('asymptotes-1',{'solution'=>true}) %>
<% hw('asymptotes-2',{'solution'=>true}) %>
<% hw('asymptotes-3',{'solution'=>true}) %>

  <% hw_block(2) %>

<% hw('bounty-hunters',{'solution'=>true}) %>
<% hw('pendulum') %>
<% hw('rod-finger-string') %>

  <% hw_block(2) %>

<% hw('x-graph-to-v-graph',{'solution'=>true}) %>

<% marg(40) %>
<%
  fig(
    'hw-rod-finger-string',
    %q{Problem \ref{hw:rod-finger-string}.}
  )  
%>
\spacebetweenfigs
<%
  fig(
    'hw-x-graph-to-v-graph',
    %q{Problem \ref{hw:x-graph-to-v-graph}.}
  )  
%>
<% end_marg %>

  <% hw_block(2) %>

<% hw('curve-sketching-rational-function-1') %>
<% hw('curve-sketching-rational-function-2') %>
<% hw('curve-sketching-rational-function-3') %>

  <% hw_block(2) %>

<% hw('prove-inflection-has-zero-2nd-deriv',{'solution'=>true}) %>

  <% hw_block(2) %>

% stupid limit tricks:

<% hw('cancel-factor') %>

% Robbin, p. 35, #4-14
% There are problems limit-prove-eps-delta-03 through limit-prove-eps-delta-11 still in git,
% but I wasn't enthusiastic enough to write up solutions for them.

<% hw('limit-prove-eps-delta-using-trick') %>

<% end_hw_sec %>

<% end_chapter %>
